"use strict";(self.webpackChunkphysical_ai_robotics=self.webpackChunkphysical_ai_robotics||[]).push([[267],{454:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/index","title":"Vision-Language-Action (VLA)","description":"Integrate vision, language, and action systems for intelligent humanoid robot interaction","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/ur/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/neha-haneef115/Physical-AI-Humanoid-Robotics-Course-Book/edit/main/docs/module-4/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Vision-Language-Action (VLA)","description":"Integrate vision, language, and action systems for intelligent humanoid robot interaction","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Machine Learning","permalink":"/ur/docs/module-3/ch3-4"},"next":{"title":"Chapter 1: VLA Fundamentals","permalink":"/ur/docs/module-4/ch4-1"}}');var o=i(4848),s=i(8453);const r={title:"Vision-Language-Action (VLA)",description:"Integrate vision, language, and action systems for intelligent humanoid robot interaction",sidebar_position:1},l="Vision-Language-Action (VLA)",a={},c=[{value:"Module Overview",id:"module-overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Chapters",id:"chapters",level:2}];function d(e){const n={br:"br",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"})}),"\n",(0,o.jsx)(n.p,{children:"Integrate vision, language, and action systems for intelligent humanoid robot interaction"}),"\n",(0,o.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Duration"}),": 55 hours",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.strong,{children:"Difficulty"}),": advanced"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Deep learning fundamentals"}),"\n",(0,o.jsx)(n.li,{children:"Natural language processing"}),"\n",(0,o.jsx)(n.li,{children:"Computer vision expertise"}),"\n",(0,o.jsx)(n.li,{children:"Robot control systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement voice-to-action systems"}),"\n",(0,o.jsx)(n.li,{children:"Design LLM-based cognitive planning"}),"\n",(0,o.jsx)(n.li,{children:"Create multi-modal interaction systems"}),"\n",(0,o.jsx)(n.li,{children:"Build complete autonomous humanoid"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"chapters",children:"Chapters"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["[Voice-to-Action Systems (Whisper)](",'<ref "ch4-1" />',")"]}),"\n",(0,o.jsxs)(n.li,{children:["[LLM-based Cognitive Planning](",'<ref "ch4-2" />',")"]}),"\n",(0,o.jsxs)(n.li,{children:["[Multi-modal Interaction Design](",'<ref "ch4-3" />',")"]}),"\n",(0,o.jsxs)(n.li,{children:["[Capstone Project - Autonomous Humanoid](",'<ref "ch4-4" />',")"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);